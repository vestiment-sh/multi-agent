{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "63a44521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smolagents in ./.env/lib/python3.13/site-packages (1.15.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.30.0 in ./.env/lib/python3.13/site-packages (from smolagents) (0.31.1)\n",
      "Requirement already satisfied: requests>=2.32.3 in ./.env/lib/python3.13/site-packages (from smolagents) (2.32.3)\n",
      "Requirement already satisfied: rich>=13.9.4 in ./.env/lib/python3.13/site-packages (from smolagents) (14.0.0)\n",
      "Requirement already satisfied: jinja2>=3.1.4 in ./.env/lib/python3.13/site-packages (from smolagents) (3.1.6)\n",
      "Requirement already satisfied: pillow>=10.0.1 in ./.env/lib/python3.13/site-packages (from smolagents) (11.2.1)\n",
      "Requirement already satisfied: python-dotenv in ./.env/lib/python3.13/site-packages (from smolagents) (1.1.0)\n",
      "Requirement already satisfied: filelock in ./.env/lib/python3.13/site-packages (from huggingface-hub>=0.30.0->smolagents) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.env/lib/python3.13/site-packages (from huggingface-hub>=0.30.0->smolagents) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.env/lib/python3.13/site-packages (from huggingface-hub>=0.30.0->smolagents) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.13/site-packages (from huggingface-hub>=0.30.0->smolagents) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.env/lib/python3.13/site-packages (from huggingface-hub>=0.30.0->smolagents) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.13/site-packages (from huggingface-hub>=0.30.0->smolagents) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in ./.env/lib/python3.13/site-packages (from huggingface-hub>=0.30.0->smolagents) (1.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.13/site-packages (from jinja2>=3.1.4->smolagents) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.13/site-packages (from requests>=2.32.3->smolagents) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.13/site-packages (from requests>=2.32.3->smolagents) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.13/site-packages (from requests>=2.32.3->smolagents) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.13/site-packages (from requests>=2.32.3->smolagents) (2025.4.26)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.env/lib/python3.13/site-packages (from rich>=13.9.4->smolagents) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.env/lib/python3.13/site-packages (from rich>=13.9.4->smolagents) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./.env/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->smolagents) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install smolagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5d6157a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[cli,torch] in ./.env/lib/python3.13/site-packages (0.31.1)\n",
      "Requirement already satisfied: filelock in ./.env/lib/python3.13/site-packages (from huggingface_hub[cli,torch]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.env/lib/python3.13/site-packages (from huggingface_hub[cli,torch]) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.env/lib/python3.13/site-packages (from huggingface_hub[cli,torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.env/lib/python3.13/site-packages (from huggingface_hub[cli,torch]) (6.0.2)\n",
      "Requirement already satisfied: requests in ./.env/lib/python3.13/site-packages (from huggingface_hub[cli,torch]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./.env/lib/python3.13/site-packages (from huggingface_hub[cli,torch]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.env/lib/python3.13/site-packages (from huggingface_hub[cli,torch]) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in ./.env/lib/python3.13/site-packages (from huggingface_hub[cli,torch]) (1.1.0)\n",
      "Requirement already satisfied: InquirerPy==0.3.4 in ./.env/lib/python3.13/site-packages (from huggingface_hub[cli,torch]) (0.3.4)\n",
      "Requirement already satisfied: torch in ./.env/lib/python3.13/site-packages (from huggingface_hub[cli,torch]) (2.7.0)\n",
      "Requirement already satisfied: safetensors[torch] in ./.env/lib/python3.13/site-packages (from huggingface_hub[cli,torch]) (0.5.3)\n",
      "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in ./.env/lib/python3.13/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli,torch]) (0.3.4)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in ./.env/lib/python3.13/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli,torch]) (3.0.51)\n",
      "Requirement already satisfied: wcwidth in ./.env/lib/python3.13/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli,torch]) (0.2.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.env/lib/python3.13/site-packages (from requests->huggingface_hub[cli,torch]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.env/lib/python3.13/site-packages (from requests->huggingface_hub[cli,torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.env/lib/python3.13/site-packages (from requests->huggingface_hub[cli,torch]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.env/lib/python3.13/site-packages (from requests->huggingface_hub[cli,torch]) (2025.4.26)\n",
      "Requirement already satisfied: setuptools in ./.env/lib/python3.13/site-packages (from torch->huggingface_hub[cli,torch]) (80.4.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.env/lib/python3.13/site-packages (from torch->huggingface_hub[cli,torch]) (1.14.0)\n",
      "Requirement already satisfied: networkx in ./.env/lib/python3.13/site-packages (from torch->huggingface_hub[cli,torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.env/lib/python3.13/site-packages (from torch->huggingface_hub[cli,torch]) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.env/lib/python3.13/site-packages (from sympy>=1.13.3->torch->huggingface_hub[cli,torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.env/lib/python3.13/site-packages (from jinja2->torch->huggingface_hub[cli,torch]) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.21.6 in ./.env/lib/python3.13/site-packages (from safetensors[torch]->huggingface_hub[cli,torch]) (2.2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'huggingface_hub[cli,torch]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b5141a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f321f0b0703468eb2f442d26a163783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4201114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"Qwen/Qwen3-0.6B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11e6e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from markdownify import markdownify\n",
    "from requests.exceptions import RequestException\n",
    "from smolagents import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def visit_webpage(url: str) -> str:\n",
    "    \"\"\"Visits a webpage at the given URL and returns its content as a markdown string.\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the webpage to visit.\n",
    "\n",
    "    Returns:\n",
    "        The content of the webpage converted to Markdown, or an error message if the request fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        # Convert the HTML content to Markdown\n",
    "        markdown_content = markdownify(response.text).strip()\n",
    "\n",
    "        # Remove multiple line breaks\n",
    "        markdown_content = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_content)\n",
    "\n",
    "        return markdown_content\n",
    "\n",
    "    except RequestException as e:\n",
    "        return f\"Error fetching the webpage: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "92e84441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face - Wikipedia\n",
      "\n",
      "[Jump to content](#bodyContent)\n",
      "\n",
      "Main menu\n",
      "\n",
      "Main menu\n",
      "\n",
      "move to sidebar\n",
      "hide\n",
      "\n",
      "Navigation\n",
      "\n",
      "* [Main page](/wiki/Main_Page \"Visit the main page [z]\")\n",
      "* [Contents](/wiki/Wikipedia:Contents \"Guides to browsing Wikipedia\")\n",
      "* [Current events](/wiki/Portal:Current_events \"Articles related to current events\")\n",
      "* [Random article](/wiki/Special:Random \"Visit a randomly selected article [x]\")\n",
      "* [About Wikipedia](/wiki/Wikipedia:About \"Learn about Wikipedia and how it works\")\n",
      "* [Conta\n"
     ]
    }
   ],
   "source": [
    "print(visit_webpage(\"https://en.wikipedia.org/wiki/Hugging_Face\")[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0a223abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import (\n",
    "    CodeAgent,\n",
    "    ToolCallingAgent,\n",
    "    InferenceClientModel,\n",
    "    WebSearchTool,\n",
    "    LiteLLMModel,\n",
    ")\n",
    "\n",
    "model = InferenceClientModel(model_id=model_id)\n",
    "\n",
    "web_agent = ToolCallingAgent(\n",
    "    tools=[WebSearchTool(), visit_webpage],\n",
    "    model=model,\n",
    "    max_steps=10,\n",
    "    name=\"web_search_agent\",\n",
    "    description=\"Runs web searches for you.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b5309374",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager_agent = CodeAgent(\n",
    "    tools=[],\n",
    "    model=model,\n",
    "    managed_agents=[web_agent],\n",
    "    additional_authorized_imports=[\"time\", \"numpy\", \"pandas\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "82b7d70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">If LLM training continues to scale up at the current rhythm until 2030, what would be the electric power in GW </span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">required to power the biggest training runs by 2030? What would that correspond to, compared to some countries?</span> <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Please provide a source for any numbers used.</span>                                                                   <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ InferenceClientModel - Qwen/Qwen3-0.6B ────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mIf LLM training continues to scale up at the current rhythm until 2030, what would be the electric power in GW \u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mrequired to power the biggest training runs by 2030? What would that correspond to, compared to some countries?\u001b[0m \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mPlease provide a source for any numbers used.\u001b[0m                                                                   \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m InferenceClientModel - Qwen/Qwen3-0.6B \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Qwen/Qwen3-0.6B is in staging mode for provider fireworks-ai. Meant for test purposes only.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Error in generating model output:</span>\n",
       "<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">402</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> Client Error: Payment Required for url: </span>\n",
       "<span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://router.huggingface.co/fireworks-ai/inference/v1/chat/completions</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> (Request ID: </span>\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">Root</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">=</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">-681fb2c0-29d465c6340df7ec4a1cb38f;</span><span style=\"color: #ffff00; text-decoration-color: #ffff00\">b7383b0c-bb68-46e5-9823-878334a47fdb</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">)</span>\n",
       "\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">You have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 2</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\"> more monthly </span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">included credits.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;31mError in generating model output:\u001b[0m\n",
       "\u001b[1;36m402\u001b[0m\u001b[1;31m Client Error: Payment Required for url: \u001b[0m\n",
       "\u001b[4;94mhttps://router.huggingface.co/fireworks-ai/inference/v1/chat/completions\u001b[0m\u001b[1;31m \u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31mRequest ID: \u001b[0m\n",
       "\u001b[1;33mRoot\u001b[0m\u001b[1;31m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;31m-681fb2c0-29d465c6340df7ec4a1cb38f;\u001b[0m\u001b[93mb7383b0c-bb68-46e5-9823-878334a47fdb\u001b[0m\u001b[1;31m)\u001b[0m\n",
       "\n",
       "\u001b[1;31mYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 2\u001b[0m\u001b[1;36m0x\u001b[0m\u001b[1;31m more monthly \u001b[0m\n",
       "\u001b[1;31mincluded credits.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 0.46 seconds]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 0.46 seconds]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AgentGenerationError",
     "evalue": "Error in generating model output:\n402 Client Error: Payment Required for url: https://router.huggingface.co/fireworks-ai/inference/v1/chat/completions (Request ID: Root=1-681fb2c0-29d465c6340df7ec4a1cb38f;b7383b0c-bb68-46e5-9823-878334a47fdb)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STEM Success Inc./.env/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STEM Success Inc./.env/lib/python3.13/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/fireworks-ai/inference/v1/chat/completions",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STEM Success Inc./.env/lib/python3.13/site-packages/smolagents/agents.py:1333\u001b[39m, in \u001b[36mCodeAgent._step_stream\u001b[39m\u001b[34m(self, memory_step)\u001b[39m\n\u001b[32m   1332\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1333\u001b[39m     chat_message: ChatMessage = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<end_code>\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mObservation:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCalling tools:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1338\u001b[39m     memory_step.model_output_message = chat_message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STEM Success Inc./.env/lib/python3.13/site-packages/smolagents/models.py:1275\u001b[39m, in \u001b[36mInferenceClientModel.generate\u001b[39m\u001b[34m(self, messages, stop_sequences, grammar, tools_to_call_from, **kwargs)\u001b[39m\n\u001b[32m   1266\u001b[39m completion_kwargs = \u001b[38;5;28mself\u001b[39m._prepare_completion_kwargs(\n\u001b[32m   1267\u001b[39m     messages=messages,\n\u001b[32m   1268\u001b[39m     stop_sequences=stop_sequences,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1273\u001b[39m     **kwargs,\n\u001b[32m   1274\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[38;5;28mself\u001b[39m.last_input_token_count = response.usage.prompt_tokens\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STEM Success Inc./.env/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:923\u001b[39m, in \u001b[36mInferenceClient.chat_completion\u001b[39m\u001b[34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[39m\n\u001b[32m    916\u001b[39m request_parameters = provider_helper.prepare_request(\n\u001b[32m    917\u001b[39m     inputs=messages,\n\u001b[32m    918\u001b[39m     parameters=parameters,\n\u001b[32m   (...)\u001b[39m\u001b[32m    921\u001b[39m     api_key=\u001b[38;5;28mself\u001b[39m.token,\n\u001b[32m    922\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m923\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inner_post\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STEM Success Inc./.env/lib/python3.13/site-packages/huggingface_hub/inference/_client.py:279\u001b[39m, in \u001b[36mInferenceClient._inner_post\u001b[39m\u001b[34m(self, request_parameters, stream)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.iter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STEM Success Inc./.env/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:482\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 402 Client Error: Payment Required for url: https://router.huggingface.co/fireworks-ai/inference/v1/chat/completions (Request ID: Root=1-681fb2c0-29d465c6340df7ec4a1cb38f;b7383b0c-bb68-46e5-9823-878334a47fdb)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAgentGenerationError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[54]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m answer = \u001b[43mmanager_agent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mIf LLM training continues to scale up at the current rhythm until 2030, what would be the electric power in GW required to power the biggest training runs by 2030? What would that correspond to, compared to some countries? Please provide a source for any numbers used.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(answer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STEM Success Inc./.env/lib/python3.13/site-packages/smolagents/agents.py:351\u001b[39m, in \u001b[36mMultiStepAgent.run\u001b[39m\u001b[34m(self, task, stream, reset, images, additional_args, max_steps)\u001b[39m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_stream(task=\u001b[38;5;28mself\u001b[39m.task, max_steps=max_steps, images=images)\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# Outputs are returned only at the end. We only look at the last step.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[-\u001b[32m1\u001b[39m].final_answer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STEM Success Inc./.env/lib/python3.13/site-packages/smolagents/agents.py:379\u001b[39m, in \u001b[36mMultiStepAgent._run_stream\u001b[39m\u001b[34m(self, task, max_steps, images)\u001b[39m\n\u001b[32m    376\u001b[39m     final_answer = el\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m AgentGenerationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    378\u001b[39m     \u001b[38;5;66;03m# Agent generation errors are not caused by a Model error but an implementation error: so we should raise them and exit.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m AgentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    381\u001b[39m     \u001b[38;5;66;03m# Other AgentError types are caused by the Model, so we should log them and iterate.\u001b[39;00m\n\u001b[32m    382\u001b[39m     action_step.error = e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STEM Success Inc./.env/lib/python3.13/site-packages/smolagents/agents.py:374\u001b[39m, in \u001b[36mMultiStepAgent._run_stream\u001b[39m\u001b[34m(self, task, max_steps, images)\u001b[39m\n\u001b[32m    370\u001b[39m action_step = ActionStep(\n\u001b[32m    371\u001b[39m     step_number=\u001b[38;5;28mself\u001b[39m.step_number, start_time=step_start_time, observations_images=images\n\u001b[32m    372\u001b[39m )\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_step\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\n\u001b[32m    376\u001b[39m     final_answer = el\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STEM Success Inc./.env/lib/python3.13/site-packages/smolagents/agents.py:397\u001b[39m, in \u001b[36mMultiStepAgent._execute_step\u001b[39m\u001b[34m(self, memory_step)\u001b[39m\n\u001b[32m    395\u001b[39m \u001b[38;5;28mself\u001b[39m.logger.log_rule(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.step_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, level=LogLevel.INFO)\n\u001b[32m    396\u001b[39m final_answer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_step\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfinal_answer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STEM Success Inc./.env/lib/python3.13/site-packages/smolagents/agents.py:1354\u001b[39m, in \u001b[36mCodeAgent._step_stream\u001b[39m\u001b[34m(self, memory_step)\u001b[39m\n\u001b[32m   1352\u001b[39m     memory_step.model_output = model_output\n\u001b[32m   1353\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1354\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m AgentGenerationError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError in generating model output:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.logger) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1356\u001b[39m \u001b[38;5;66;03m### Parse output ###\u001b[39;00m\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mAgentGenerationError\u001b[39m: Error in generating model output:\n402 Client Error: Payment Required for url: https://router.huggingface.co/fireworks-ai/inference/v1/chat/completions (Request ID: Root=1-681fb2c0-29d465c6340df7ec4a1cb38f;b7383b0c-bb68-46e5-9823-878334a47fdb)\n\nYou have exceeded your monthly included credits for Inference Providers. Subscribe to PRO to get 20x more monthly included credits."
     ]
    }
   ],
   "source": [
    "answer = manager_agent.run(\"If LLM training continues to scale up at the current rhythm until 2030, what would be the electric power in GW required to power the biggest training runs by 2030? What would that correspond to, compared to some countries? Please provide a source for any numbers used.\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8f9a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
